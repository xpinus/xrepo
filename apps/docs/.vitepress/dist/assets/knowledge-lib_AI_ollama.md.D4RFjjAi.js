import{a3 as s,t as l,ao as i,q as t}from"./chunks/framework.Dt9YBBJv.js";const F=JSON.parse('{"title":"Ollama","description":"","frontmatter":{},"headers":[],"relativePath":"knowledge-lib/AI/ollama.md","filePath":"knowledge-lib/AI/ollama.md"}'),e={name:"knowledge-lib/AI/ollama.md"};function n(h,a,p,k,r,o){return t(),l("div",null,a[0]||(a[0]=[i(`<h1 id="ollama" tabindex="-1">Ollama <a class="header-anchor" href="#ollama" aria-label="Permalink to &quot;Ollama&quot;">​</a></h1><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pull</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llama3.1</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">   # Pull a model</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> rm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llama3.1</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cp</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llama3.1</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> my-model</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> show</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llama3.1</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  #  Show model information</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> list</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # List models on your computer</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llama3.1</span></span></code></pre></div><p><strong>修改ollama的模型默认存放路径</strong><a href="https://blog.csdn.net/LJX_ahut/article/details/139310401" target="_blank" rel="noreferrer">https://blog.csdn.net/LJX_ahut/article/details/139310401</a><a href="https://blog.csdn.net/LJX_ahut/article/details/139310401" target="_blank" rel="noreferrer">https://blog.csdn.net/LJX_ahut/article/details/139310401</a></p><blockquote><p>open-wenui 为olloma打造的web交互界面</p></blockquote>`,4)]))}const m=s(e,[["render",n]]);export{F as __pageData,m as default};
